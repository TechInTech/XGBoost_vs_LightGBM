

{'n_train': 500000, 'num_leaves': 32, 'max_depth': 5}
training XGBoost
EQBIN_dw: 8.35443377494812 seconds
EQBIN_lg: 7.960616827011108 seconds
training LightGBM
LightGBM: 5.458090782165527 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.675223  0.675223  0.675159
15   0.538657  0.538657  0.536850
30   0.489796  0.489796  0.487917
45   0.461254  0.461254  0.460996
60   0.443328  0.443328  0.443085
75   0.422786  0.422786  0.427510
90   0.407351  0.407351  0.416744
105  0.392894  0.392894  0.404913
120  0.385819  0.385819  0.393944
135  0.379876  0.379876  0.385745
150  0.373250  0.373250  0.378372
165  0.366858  0.366858  0.370963
180  0.361597  0.361597  0.363289
195  0.357106  0.357106  0.357302

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.675327  0.675327  0.675282
15   0.540042  0.540042  0.538393
30   0.491851  0.491851  0.490047
45   0.463598  0.463598  0.463464
60   0.446067  0.446067  0.446023
75   0.426136  0.426136  0.430857
90   0.411187  0.411187  0.420516
105  0.397151  0.397151  0.409005
120  0.390497  0.390497  0.398496
135  0.384954  0.384954  0.390699
150  0.378667  0.378667  0.383540
165  0.372594  0.372594  0.376522
180  0.367742  0.367742  0.369207
195  0.363628  0.363628  0.363678

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         30        30   29
75         30        30   32
90         30        30   32
105        32        32   31
120        31        31   32
135        32        32   31
150        32        32   32
165        32        32   32
180        31        31   32
195        30        30   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.125244  0.125244  0.139206
f1   0.077363  0.077363  0.043795
f7   0.073242  0.073242  0.098834
f17  0.070430  0.070430  0.055689
f5   0.068089  0.068089  0.054752


{'n_train': 500000, 'num_leaves': 32, 'max_depth': 10}
training XGBoost
EQBIN_dw: 8.394717454910278 seconds
EQBIN_lg: 7.9799158573150635 seconds
training LightGBM
LightGBM: 6.295960426330566 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.675223  0.669968  0.669964
15   0.538657  0.507373  0.504056
30   0.489796  0.452641  0.450193
45   0.461254  0.424939  0.421044
60   0.441973  0.405291  0.400745
75   0.424671  0.389522  0.387223
90   0.409568  0.373764  0.376561
105  0.402111  0.363740  0.365810
120  0.391036  0.355245  0.356653
135  0.383902  0.347198  0.348178
150  0.375794  0.340875  0.340077
165  0.367041  0.334307  0.336314
180  0.362225  0.329864  0.331132
195  0.356370  0.322570  0.325978

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.675327  0.670061  0.670104
15   0.540042  0.508494  0.505676
30   0.491851  0.454354  0.452610
45   0.463598  0.427174  0.423845
60   0.444825  0.408179  0.404021
75   0.428041  0.393146  0.391019
90   0.413417  0.377977  0.380836
105  0.406221  0.368565  0.370526
120  0.395504  0.360624  0.361898
135  0.388673  0.353099  0.353956
150  0.381016  0.347152  0.346390
165  0.372565  0.341059  0.343069
180  0.368032  0.337215  0.338456
195  0.362488  0.330534  0.333815

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         32        32   32
90         32        32   32
105        32        32   32
120        32        32   32
135        32        32   32
150        32        32   32
165        32        32   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.125269  0.127673  0.136560
f7   0.077845  0.064583  0.094765
f1   0.072288  0.069269  0.037269
f9   0.067771  0.071354  0.085388
f17  0.067761  0.063767  0.052060


{'n_train': 500000, 'num_leaves': 256, 'max_depth': 10}
training XGBoost
EQBIN_dw: 18.283043146133423 seconds
EQBIN_lg: 17.653311729431152 seconds
training LightGBM
LightGBM: 11.72837519645691 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.660419  0.654859  0.655016
15   0.445656  0.412285  0.411963
30   0.386099  0.343984  0.345963
45   0.345863  0.314970  0.314964
60   0.324551  0.295087  0.297044
75   0.306972  0.278582  0.279776
90   0.294403  0.267757  0.265181
105  0.280862  0.259438  0.256810
120  0.270997  0.250853  0.247393
135  0.264641  0.242579  0.238975
150  0.255665  0.234124  0.231507
165  0.248372  0.225951  0.224825
180  0.239119  0.218516  0.218838
195  0.234054  0.214080  0.213493

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.660835  0.655368  0.655577
15   0.449799  0.417807  0.417653
30   0.393282  0.353205  0.355185
45   0.355624  0.327243  0.326952
60   0.336659  0.310049  0.311700
75   0.321291  0.296213  0.297243
90   0.310871  0.287915  0.285631
105  0.299856  0.282387  0.279880
120  0.292293  0.276905  0.273602
135  0.288390  0.271804  0.268588
150  0.281990  0.266263  0.264452
165  0.277379  0.261208  0.261003
180  0.270612  0.256418  0.258080
195  0.268253  0.255184  0.255280

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         256       256  256
15        256       256  256
30        256       256  256
45        256       256  256
60        256       256  205
75        256       256  256
90        256       218  256
105       256       256  256
120       256       256  256
135       256       256  256
150       256       256  256
165       256       256  256
180       256       256  256
195       256       217  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.118651  0.113403  0.117106
f9   0.073339  0.066822  0.069162
f7   0.068624  0.064389  0.085848
f17  0.059297  0.055403  0.048813
f31  0.059139  0.067048  0.069502


{'n_train': 500000, 'num_leaves': 1024, 'max_depth': 10}
training XGBoost
EQBIN_dw: 26.065101861953735 seconds
EQBIN_lg: 25.745198011398315 seconds
training LightGBM
LightGBM: 14.126919031143188 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.651639  0.651639  0.651787
15   0.390788  0.390788  0.392704
30   0.324340  0.324340  0.327585
45   0.296892  0.296892  0.296369
60   0.274448  0.274448  0.275948
75   0.257760  0.257760  0.259194
90   0.244864  0.244864  0.247598
105  0.234966  0.234966  0.232853
120  0.223411  0.223411  0.225359
135  0.214793  0.214793  0.215399
150  0.208904  0.208904  0.205137
165  0.200966  0.200966  0.200639
180  0.194875  0.194875  0.194603
195  0.189971  0.189971  0.188973

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.652533  0.652533  0.652749
15   0.401175  0.401175  0.403117
30   0.340391  0.340391  0.343289
45   0.316944  0.316944  0.316627
60   0.298326  0.298326  0.299941
75   0.285349  0.285349  0.286828
90   0.276250  0.276250  0.278605
105  0.270693  0.270693  0.268875
120  0.263494  0.263494  0.264434
135  0.259340  0.259340  0.259011
150  0.257408  0.257408  0.253517
165  0.253312  0.253312  0.252260
180  0.250792  0.250792  0.250566
195  0.248831  0.248831  0.248684

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         852       852  844
15        745       745  782
30        521       521  490
45        413       413  497
60        723       723  566
75        172       172  264
90        342       342  484
105       302       302  347
120       759       759  290
135       373       373  525
150       399       399  583
165       400       400  459
180       578       578  153
195       249       249  181

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.115447  0.115447  0.113983
f9   0.068445  0.068445  0.068954
f7   0.063394  0.063394  0.083946
f31  0.063055  0.063055  0.068456
f10  0.057661  0.057661  0.093736


{'n_train': 500000, 'num_leaves': 32, 'max_depth': 15}
training XGBoost
EQBIN_dw: 8.343898057937622 seconds
EQBIN_lg: 8.034099578857422 seconds
training LightGBM
LightGBM: 6.365386009216309 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.675223  0.669968  0.669964
15   0.538657  0.504339  0.505071
30   0.489796  0.448982  0.449470
45   0.461254  0.419500  0.418257
60   0.441973  0.399314  0.397380
75   0.424671  0.384352  0.380460
90   0.409568  0.371749  0.368609
105  0.402111  0.360344  0.359724
120  0.391036  0.349898  0.351468
135  0.383902  0.342412  0.342732
150  0.375794  0.336676  0.335642
165  0.367041  0.332410  0.329198
180  0.362225  0.326543  0.323556
195  0.356370  0.321183  0.318037

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.675327  0.670061  0.670104
15   0.540042  0.505683  0.506502
30   0.491851  0.450852  0.451511
45   0.463598  0.421869  0.420962
60   0.444825  0.402349  0.400770
75   0.428041  0.388088  0.384485
90   0.413417  0.376058  0.373277
105  0.406221  0.365294  0.364934
120  0.395504  0.355332  0.357278
135  0.388673  0.348370  0.349151
150  0.381016  0.343221  0.342505
165  0.372565  0.339496  0.336528
180  0.368032  0.334133  0.331519
195  0.362488  0.329340  0.326478

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         32        32   32
90         32        32   32
105        32        32   32
120        32        32   32
135        32        32   32
150        32        32   32
165        32        32   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.125269  0.126979  0.136217
f7   0.077845  0.066851  0.091529
f1   0.072288  0.081684  0.038298
f9   0.067771  0.071102  0.083204
f17  0.067761  0.065601  0.050563


{'n_train': 500000, 'num_leaves': 256, 'max_depth': 15}
training XGBoost
EQBIN_dw: 18.16001605987549 seconds
EQBIN_lg: 17.334385871887207 seconds
training LightGBM
LightGBM: 12.096617937088013 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.660419  0.653653  0.653966
15   0.445656  0.396437  0.397702
30   0.386099  0.319544  0.323089
45   0.345863  0.290004  0.292334
60   0.324551  0.271027  0.274777
75   0.307560  0.257728  0.259717
90   0.293811  0.246880  0.250524
105  0.281713  0.240516  0.244159
120  0.270883  0.234158  0.238195
135  0.260389  0.229712  0.232565
150  0.251743  0.222241  0.228049
165  0.244601  0.215327  0.218883
180  0.239018  0.210774  0.213127
195  0.234650  0.206706  0.208882

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.660835  0.654242  0.654562
15   0.449799  0.402374  0.404126
30   0.393282  0.329378  0.332842
45   0.355624  0.303014  0.305400
60   0.336659  0.287085  0.290906
75   0.321924  0.277238  0.279335
90   0.310479  0.269961  0.273815
105  0.300796  0.267413  0.270971
120  0.292384  0.264856  0.268420
135  0.284450  0.264075  0.266286
150  0.278564  0.259935  0.265199
165  0.274273  0.256133  0.259088
180  0.271338  0.255453  0.256706
195  0.269654  0.254698  0.255761

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         256       256  256
15        256       256  256
30        256       256  256
45        256       256  256
60        256       256  256
75        256       256  256
90        256       256  256
105       256       256  256
120       256       256  256
135       256       256  256
150       256       256  256
165       256       256  256
180       256       256  256
195       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.119129  0.113184  0.114545
f9   0.072751  0.068910  0.069879
f7   0.069169  0.062379  0.084398
f10  0.059257  0.058302  0.094745
f17  0.058706  0.051032  0.048781


{'n_train': 500000, 'num_leaves': 1024, 'max_depth': 15}
training XGBoost
EQBIN_dw: 42.61528515815735 seconds
EQBIN_lg: 41.44998836517334 seconds
training LightGBM
LightGBM: 23.301652669906616 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.650728  0.643126  0.643179
15   0.381140  0.336316  0.336360
30   0.305366  0.252789  0.253922
45   0.266276  0.218436  0.219400
60   0.239462  0.201520  0.201049
75   0.220427  0.184453  0.186899
90   0.202603  0.173328  0.173162
105  0.191875  0.161192  0.162216
120  0.180153  0.149300  0.150966
135  0.167028  0.139448  0.140027
150  0.157270  0.131558  0.130374
165  0.149207  0.122681  0.121893
180  0.141134  0.114982  0.113722
195  0.132256  0.107239  0.106911

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.651801  0.644820  0.644782
15   0.393680  0.353735  0.353742
30   0.326315  0.282074  0.283002
45   0.294495  0.258017  0.258784
60   0.274747  0.249367  0.248674
75   0.262881  0.242585  0.244103
90   0.253732  0.240142  0.240589
105  0.250122  0.236741  0.238340
120  0.246481  0.233530  0.235144
135  0.241153  0.231894  0.232239
150  0.238898  0.230694  0.230144
165  0.237557  0.229376  0.229205
180  0.235415  0.228396  0.227819
195  0.232610  0.227252  0.226846

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        1024      1024  1024
15       1024      1024  1024
30       1024      1024  1024
45       1024      1024  1024
60       1024       817  1024
75       1024      1024  1024
90       1024      1024   659
105      1024      1024  1024
120      1024      1024  1024
135      1024      1024  1024
150      1024      1024  1024
165       629      1024  1024
180      1024      1024   573
195      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.104525  0.099508  0.104424
f9   0.065486  0.063380  0.061893
f7   0.063335  0.059417  0.073818
f31  0.063033  0.064322  0.069396
f10  0.058011  0.057214  0.088216


{'n_train': 500000, 'num_leaves': 4096, 'max_depth': 15}
training XGBoost
EQBIN_dw: 77.16223955154419 seconds
EQBIN_lg: 76.30810523033142 seconds
training LightGBM
LightGBM: 35.530843019485474 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.638249  0.635119  0.635125
15   0.304004  0.287983  0.287071
30   0.210452  0.192443  0.194116
45   0.178598  0.161888  0.163257
60   0.161279  0.149479  0.150281
75   0.144680  0.133556  0.135912
90   0.131739  0.119178  0.122579
105  0.118691  0.106468  0.111794
120  0.104918  0.096595  0.099763
135  0.095043  0.087608  0.090105
150  0.086893  0.079124  0.081695
165  0.078863  0.072085  0.076019
180  0.071321  0.067092  0.067041
195  0.065756  0.059248  0.060132

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.641995  0.639688  0.639393
15   0.342940  0.332860  0.331907
30   0.272719  0.263850  0.263735
45   0.252559  0.245412  0.244793
60   0.243413  0.239214  0.238316
75   0.237795  0.232962  0.234146
90   0.234783  0.229043  0.231606
105  0.232045  0.226292  0.229299
120  0.229127  0.225153  0.226043
135  0.227567  0.224299  0.225167
150  0.225802  0.222758  0.223646
165  0.224652  0.221945  0.223168
180  0.224101  0.221395  0.221670
195  0.223289  0.220479  0.221036

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        4096      4096  4096
15       4096      4096  4096
30       3185      3649  2928
45       1699      2252  2561
60       1362       978  1611
75        430       584  1386
90       4096       516  1373
105      1926      1621   934
120      3266       565  1203
135      4096      2373   899
150      4096       558  1198
165       801      2429   559
180      3162       909  2271
195       762      3391  1340

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.097489  0.096947  0.100727
f9   0.062846  0.061169  0.059679
f31  0.061997  0.062953  0.070957
f7   0.057490  0.057339  0.069794
f10  0.055121  0.054940  0.082049


{'n_train': 500000, 'num_leaves': 16384, 'max_depth': 15}
training XGBoost
EQBIN_dw: 84.33343887329102 seconds
EQBIN_lg: 83.84466862678528 seconds
training LightGBM
LightGBM: 40.08955264091492 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.633882  0.633882  0.634071
15   0.281393  0.281393  0.282803
30   0.188958  0.188958  0.189968
45   0.161164  0.161164  0.159464
60   0.146568  0.146568  0.147352
75   0.134437  0.134437  0.131065
90   0.117677  0.117677  0.119656
105  0.106300  0.106300  0.109472
120  0.096707  0.096707  0.096732
135  0.087125  0.087125  0.088741
150  0.076294  0.076294  0.078433
165  0.069482  0.069482  0.070104
180  0.063461  0.063461  0.064631
195  0.056269  0.056269  0.059015

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.639428  0.639428  0.639166
15   0.331964  0.331964  0.331552
30   0.264912  0.264912  0.263853
45   0.247136  0.247136  0.245134
60   0.239228  0.239228  0.238791
75   0.235107  0.235107  0.234348
90   0.231170  0.231170  0.232085
105  0.229382  0.229382  0.229247
120  0.228073  0.228073  0.225302
135  0.226080  0.226080  0.224208
150  0.224066  0.224066  0.222574
165  0.223107  0.223107  0.221172
180  0.222768  0.222768  0.220269
195  0.221728  0.221728  0.220145

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        6423      6423  6329
15       5409      5409  6086
30       3025      3025  3144
45       1478      1478  2142
60       1396      1396   891
75       1126      1126   517
90       2916      2916   785
105      1637      1637  2464
120      1070      1070  5170
135      1788      1788  2338
150      3349      3349  1875
165      2555      2555  1880
180      1466      1466  1075
195      1473      1473  2397

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.096565  0.096565  0.101058
f9   0.063090  0.063090  0.059608
f31  0.062483  0.062483  0.069464
f7   0.056901  0.056901  0.070397
f10  0.055802  0.055802  0.082526


{'n_train': 500000, 'num_leaves': 32, 'max_depth': 20}
training XGBoost
EQBIN_dw: 8.251190662384033 seconds
EQBIN_lg: 8.110783815383911 seconds
training LightGBM
LightGBM: 6.207611322402954 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.675223  0.669968  0.669964
15   0.538657  0.504339  0.505071
30   0.489796  0.448949  0.447672
45   0.461254  0.420841  0.419927
60   0.441973  0.402737  0.400765
75   0.424671  0.389002  0.383045
90   0.409568  0.375777  0.371528
105  0.402111  0.365269  0.361442
120  0.391036  0.357292  0.355231
135  0.383902  0.349877  0.348110
150  0.375794  0.342334  0.342293
165  0.367041  0.334894  0.335972
180  0.362225  0.328755  0.330748
195  0.356370  0.323611  0.325765

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.675327  0.670061  0.670104
15   0.540042  0.505683  0.506502
30   0.491851  0.450813  0.449614
45   0.463598  0.423050  0.422468
60   0.444825  0.405542  0.403883
75   0.428041  0.392532  0.386883
90   0.413417  0.379991  0.375897
105  0.406221  0.370130  0.366401
120  0.395504  0.362799  0.360631
135  0.388673  0.355917  0.353985
150  0.381016  0.348882  0.348559
165  0.372565  0.342056  0.342742
180  0.368032  0.336465  0.338064
195  0.362488  0.331837  0.333689

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         32        32   32
90         32        32   32
105        32        32   32
120        32        32   32
135        32        32   32
150        32        32   32
165        32        32   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.125269  0.125244  0.137626
f7   0.077845  0.064981  0.089094
f1   0.072288  0.077573  0.038883
f9   0.067771  0.069495  0.087749
f17  0.067761  0.068597  0.049599


{'n_train': 500000, 'num_leaves': 256, 'max_depth': 20}
training XGBoost
EQBIN_dw: 18.044599533081055 seconds
EQBIN_lg: 16.750251054763794 seconds
training LightGBM
LightGBM: 12.175575256347656 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.660419  0.653653  0.653966
15   0.445656  0.395474  0.396297
30   0.386099  0.317459  0.318905
45   0.345863  0.283433  0.284111
60   0.324551  0.263905  0.263636
75   0.307560  0.253387  0.253100
90   0.293811  0.245690  0.246459
105  0.281713  0.239653  0.238939
120  0.270883  0.233274  0.232942
135  0.260389  0.227804  0.227143
150  0.251743  0.222555  0.220927
165  0.244601  0.216768  0.214362
180  0.239018  0.211587  0.209370
195  0.234650  0.205142  0.203138

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.660835  0.654242  0.654562
15   0.449799  0.401607  0.402703
30   0.393282  0.327482  0.328881
45   0.355624  0.296832  0.297446
60   0.336659  0.280721  0.280329
75   0.321924  0.273877  0.273566
90   0.310479  0.270201  0.270761
105  0.300796  0.267648  0.266859
120  0.292384  0.264845  0.264180
135  0.284450  0.262830  0.261841
150  0.278564  0.260868  0.258813
165  0.274273  0.257775  0.255572
180  0.271338  0.255507  0.253982
195  0.269654  0.252207  0.250772

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         256       256  256
15        256       256  256
30        256       256  256
45        256       256  256
60        256       256  256
75        256       256  256
90        256       256  256
105       256       256  256
120       256       256  256
135       256       256  256
150       256       256  256
165       256       256  256
180       256       256  256
195       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.119129  0.113740  0.113617
f9   0.072751  0.070219  0.069512
f7   0.069169  0.061658  0.083999
f10  0.059257  0.057785  0.095079
f17  0.058706  0.052120  0.048212


{'n_train': 500000, 'num_leaves': 1024, 'max_depth': 20}
training XGBoost
EQBIN_dw: 43.37070918083191 seconds
EQBIN_lg: 42.23586344718933 seconds
training LightGBM
LightGBM: 25.0202853679657 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.650728  0.642855  0.642935
15   0.381140  0.332252  0.331822
30   0.305366  0.246955  0.246637
45   0.266276  0.211213  0.208533
60   0.239386  0.189230  0.186298
75   0.219788  0.174473  0.171477
90   0.205906  0.161722  0.158182
105  0.192419  0.150549  0.145442
120  0.178247  0.139181  0.135364
135  0.166934  0.129020  0.125568
150  0.156319  0.119037  0.116261
165  0.147519  0.110326  0.108521
180  0.138963  0.102022  0.100627
195  0.131748  0.094655  0.093798

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.651801  0.644539  0.644562
15   0.393680  0.350114  0.349590
30   0.326315  0.277369  0.276677
45   0.294495  0.252317  0.249895
60   0.274656  0.242146  0.238810
75   0.262760  0.238951  0.235687
90   0.257027  0.236805  0.233645
105  0.251768  0.234775  0.230464
120  0.244885  0.231833  0.229435
135  0.240849  0.230303  0.227629
150  0.236798  0.228482  0.226249
165  0.234899  0.227174  0.225573
180  0.232987  0.225811  0.224263
195  0.231599  0.224417  0.223349

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        1024      1024  1024
15       1024      1024  1024
30       1024      1024  1024
45       1024      1024  1024
60       1024      1024  1024
75       1024      1024  1024
90       1024      1024  1024
105      1024      1024  1024
120      1024      1024  1024
135      1024      1024  1024
150      1024      1024  1024
165      1024      1024  1024
180      1024      1024  1024
195      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.107149  0.100070  0.102359
f9   0.065546  0.064279  0.061285
f7   0.063039  0.058672  0.075125
f31  0.061679  0.062504  0.067027
f10  0.058114  0.055391  0.088461


{'n_train': 500000, 'num_leaves': 4096, 'max_depth': 20}
training XGBoost
EQBIN_dw: 112.64230489730835 seconds
EQBIN_lg: 113.81252646446228 seconds
training LightGBM
LightGBM: 54.840999364852905 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.638249  0.632386  0.632352
15   0.304004  0.263888  0.264585
30   0.208051  0.162379  0.163065
45   0.159551  0.120920  0.119676
60   0.139336  0.104461  0.105292
75   0.118094  0.089495  0.089668
90   0.098598  0.073592  0.076596
105  0.082876  0.060150  0.063237
120  0.068826  0.048534  0.053261
135  0.057697  0.038929  0.045386
150  0.046681  0.033873  0.035827
165  0.038105  0.026152  0.028536
180  0.032757  0.021129  0.023522
195  0.028061  0.017894  0.019532

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.641995  0.637384  0.637233
15   0.342940  0.317612  0.318010
30   0.271828  0.249819  0.250569
45   0.244324  0.230677  0.230760
60   0.235702  0.225052  0.225733
75   0.230296  0.222071  0.222771
90   0.226332  0.219748  0.220883
105  0.224279  0.217470  0.218930
120  0.222867  0.216145  0.217415
135  0.221517  0.215538  0.216668
150  0.220004  0.215756  0.216313
165  0.219737  0.216183  0.216502
180  0.219869  0.217151  0.217506
195  0.220236  0.218472  0.218493

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        4096      4096  4096
15       4096      4096  4096
30       4096      4096  4096
45       4096      2562  4096
60       3032      4096  1401
75       4096      2808  1623
90       4096      4096  3056
105      2116      4096  3207
120      4096      3098  1765
135      3236      3948  1663
150      3550      2263  4096
165      2794      4096  4096
180      3912      4096  2983
195      1363      3579  3886

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.094020  0.094717  0.096780
f31  0.062246  0.064052  0.067872
f9   0.062123  0.061928  0.057525
f7   0.057860  0.056893  0.068872
f10  0.055850  0.055396  0.082196


{'n_train': 500000, 'num_leaves': 16384, 'max_depth': 20}
training XGBoost
EQBIN_dw: 149.36154294013977 seconds
EQBIN_lg: 148.74196434020996 seconds
training LightGBM
LightGBM: 67.73245096206665 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.623334  0.623334  0.623983
15   0.211492  0.211492  0.215349
30   0.107891  0.107891  0.112423
45   0.073119  0.073119  0.076752
60   0.062054  0.062054  0.063109
75   0.052974  0.052974  0.056047
90   0.045888  0.045888  0.048696
105  0.039617  0.039617  0.040457
120  0.030806  0.030806  0.032939
135  0.024551  0.024551  0.026221
150  0.020344  0.020344  0.022461
165  0.017579  0.017579  0.018829
180  0.015581  0.015581  0.016353
195  0.013935  0.013935  0.014595

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.635463  0.635463  0.634962
15   0.312379  0.312379  0.312411
30   0.246772  0.246772  0.247033
45   0.229131  0.229131  0.229259
60   0.224558  0.224558  0.223636
75   0.221997  0.221997  0.221532
90   0.220967  0.220967  0.220282
105  0.220308  0.220308  0.219202
120  0.219843  0.219843  0.218933
135  0.220263  0.220263  0.218874
150  0.220967  0.220967  0.219665
165  0.221863  0.221863  0.220493
180  0.222788  0.222788  0.221306
195  0.223667  0.223667  0.222351

Leaf counts
     EQBIN_dw  EQBIN_lg    LGB
0       15893     15893  15323
15      12647     12647  13152
30       9636      9636   8802
45       4319      4319   5258
60       1323      1323   2963
75       1905      1905   2146
90       4906      4906   3795
105      5394      5394   6417
120      4263      4263   4062
135      4633      4633   1636
150      4260      4260   4865
165      3711      3711   2321
180      1044      1044   2314
195      1046      1046   2506

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.096439  0.096439  0.099126
f31  0.064588  0.064588  0.069014
f9   0.060195  0.060195  0.056994
f7   0.055649  0.055649  0.067899
f30  0.053693  0.053693  0.047276


{'n_train': 1000000, 'num_leaves': 32, 'max_depth': 5}
training XGBoost
EQBIN_dw: 16.151307106018066 seconds
EQBIN_lg: 15.69100832939148 seconds
training LightGBM
LightGBM: 11.326506614685059 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.670712  0.670712  0.670690
15   0.519764  0.519764  0.517547
30   0.465616  0.465616  0.462235
45   0.437321  0.437321  0.435981
60   0.416064  0.416064  0.415267
75   0.401604  0.401604  0.399342
90   0.387141  0.387141  0.386791
105  0.377943  0.377943  0.377562
120  0.371751  0.371751  0.371134
135  0.363106  0.363106  0.362164
150  0.356368  0.356368  0.355716
165  0.351519  0.351519  0.350397
180  0.344373  0.344373  0.344984
195  0.338864  0.338864  0.340146

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.670815  0.670815  0.670768
15   0.520467  0.520467  0.518067
30   0.466785  0.466785  0.463390
45   0.439002  0.439002  0.437522
60   0.418094  0.418094  0.417141
75   0.403941  0.403941  0.401528
90   0.389714  0.389714  0.389355
105  0.380822  0.380822  0.380342
120  0.374910  0.374910  0.374079
135  0.366469  0.366469  0.365226
150  0.360049  0.360049  0.359005
165  0.355397  0.355397  0.353831
180  0.348419  0.348419  0.348625
195  0.343219  0.343219  0.343988

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         32        32   32
90         29        29   32
105        32        32   32
120        32        32   32
135        31        31   32
150        32        32   32
165        29        29   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.141327  0.141327  0.134151
f15  0.127395  0.127395  0.119172
f26  0.086050  0.086050  0.082649
f27  0.082321  0.082321  0.075474
f12  0.070329  0.070329  0.061019


{'n_train': 1000000, 'num_leaves': 32, 'max_depth': 10}
training XGBoost
EQBIN_dw: 16.037052631378174 seconds
EQBIN_lg: 15.468916416168213 seconds
training LightGBM
LightGBM: 13.30767560005188 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.670712  0.668968  0.668840
15   0.519764  0.498551  0.498702
30   0.465616  0.435091  0.435797
45   0.437321  0.404291  0.402613
60   0.416064  0.383384  0.382852
75   0.401602  0.366779  0.366588
90   0.387134  0.356035  0.356626
105  0.376121  0.347044  0.346900
120  0.368930  0.339186  0.340142
135  0.361933  0.332896  0.333221
150  0.357223  0.327741  0.327381
165  0.349977  0.322623  0.322336
180  0.344920  0.318604  0.316594
195  0.339195  0.313570  0.311414

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.670815  0.669090  0.668960
15   0.520467  0.499141  0.499510
30   0.466785  0.436483  0.437333
45   0.439002  0.406118  0.404662
60   0.418094  0.385431  0.385206
75   0.403939  0.369090  0.369147
90   0.389707  0.358722  0.359459
105  0.379062  0.349946  0.349918
120  0.372137  0.342284  0.343417
135  0.365405  0.336256  0.336673
150  0.360922  0.331347  0.331072
165  0.353928  0.326432  0.326268
180  0.349107  0.322587  0.320845
195  0.343517  0.317807  0.315849

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         32        32   32
90         32        32   32
105        32        32   32
120        32        32   32
135        32        32   32
150        32        32   32
165        32        32   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.140107  0.126506  0.118833
f15  0.122935  0.108628  0.105144
f26  0.090022  0.080820  0.083973
f27  0.086085  0.107656  0.071126
f12  0.068082  0.078310  0.059453


{'n_train': 1000000, 'num_leaves': 256, 'max_depth': 10}
training XGBoost
EQBIN_dw: 29.211477279663086 seconds
EQBIN_lg: 28.323208332061768 seconds
training LightGBM
LightGBM: 23.363593101501465 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.658978  0.654731  0.654782
15   0.436275  0.409136  0.407353
30   0.364068  0.335276  0.332471
45   0.330131  0.300765  0.301992
60   0.309972  0.284512  0.285799
75   0.294529  0.270522  0.271131
90   0.285495  0.258248  0.257753
105  0.274485  0.247846  0.247697
120  0.267220  0.240443  0.241119
135  0.258687  0.233898  0.232676
150  0.250579  0.228270  0.227312
165  0.245805  0.222656  0.223417
180  0.238242  0.218731  0.219868
195  0.231710  0.215134  0.216105

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.659296  0.655093  0.655152
15   0.439298  0.412880  0.410991
30   0.369188  0.341609  0.338589
45   0.336710  0.308982  0.309876
60   0.317876  0.294392  0.295134
75   0.303677  0.281850  0.281958
90   0.295721  0.271192  0.270131
105  0.285901  0.262132  0.261600
120  0.279843  0.256325  0.256700
135  0.272573  0.251479  0.249916
150  0.265802  0.247573  0.246271
165  0.262552  0.243432  0.243981
180  0.256215  0.241342  0.242364
195  0.251042  0.239506  0.240504

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         256       256  256
15        256       256  256
30        256       256  256
45        256       256  256
60        256       256  256
75        256       256  256
90        256       256  256
105       256       256  256
120       256       256  256
135       256       256  256
150       256       256  256
165       256       256  256
180       256       256  256
195       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f15  0.110276  0.106002  0.094652
f23  0.109606  0.109734  0.106658
f27  0.092350  0.090317  0.057798
f26  0.078407  0.072730  0.074652
f12  0.066186  0.065171  0.052247


{'n_train': 1000000, 'num_leaves': 1024, 'max_depth': 10}
training XGBoost
EQBIN_dw: 42.15245056152344 seconds
EQBIN_lg: 40.868220806121826 seconds
training LightGBM
LightGBM: 28.970285654067993 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.651142  0.651142  0.651270
15   0.383524  0.383524  0.383824
30   0.308016  0.308016  0.308515
45   0.274393  0.274393  0.275501
60   0.258309  0.258309  0.260013
75   0.246340  0.246340  0.245621
90   0.234777  0.234777  0.235061
105  0.225060  0.225060  0.223768
120  0.217585  0.217585  0.215586
135  0.210084  0.210084  0.207689
150  0.200119  0.200119  0.200345
165  0.194310  0.194310  0.196413
180  0.191428  0.191428  0.191419
195  0.186787  0.186787  0.182832

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.651907  0.651907  0.652047
15   0.391024  0.391024  0.391438
30   0.320003  0.320003  0.320869
45   0.289681  0.289681  0.291062
60   0.275817  0.275817  0.277569
75   0.265776  0.265776  0.265419
90   0.256561  0.256561  0.257145
105  0.249292  0.249292  0.248699
120  0.244795  0.244795  0.243832
135  0.239420  0.239420  0.239008
150  0.232710  0.232710  0.234916
165  0.229940  0.229940  0.233738
180  0.229551  0.229551  0.231380
195  0.227608  0.227608  0.225549

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         921       921  929
15        887       887  954
30        853       853  656
45        605       605  455
60        606       606  687
75        807       807  514
90        524       524  319
105       340       340  643
120       704       704  408
135       248       248  437
150       422       422  543
165       749       749  457
180       232       232  484
195       567       567  762

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.105729  0.105729  0.103563
f15  0.102879  0.102879  0.094337
f27  0.085082  0.085082  0.058169
f26  0.074980  0.074980  0.070556
f12  0.061103  0.061103  0.051397


{'n_train': 1000000, 'num_leaves': 32, 'max_depth': 15}
training XGBoost
EQBIN_dw: 15.697230339050293 seconds
EQBIN_lg: 15.757560014724731 seconds
training LightGBM
LightGBM: 13.471305131912231 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.670712  0.668968  0.668840
15   0.519764  0.497579  0.498695
30   0.465616  0.433017  0.436759
45   0.437321  0.400551  0.405310
60   0.416064  0.379374  0.384706
75   0.401602  0.367048  0.369787
90   0.387134  0.355086  0.358808
105  0.376121  0.345083  0.347798
120  0.368930  0.337904  0.340338
135  0.361933  0.330417  0.331772
150  0.357223  0.323761  0.324435
165  0.349977  0.317371  0.318875
180  0.344920  0.312428  0.314737
195  0.339195  0.308345  0.310399

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.670815  0.669090  0.668960
15   0.520467  0.498277  0.499392
30   0.466785  0.434389  0.438104
45   0.439002  0.402278  0.406981
60   0.418094  0.381363  0.386806
75   0.403939  0.369384  0.372112
90   0.389707  0.357700  0.361414
105  0.379062  0.347979  0.350778
120  0.372137  0.341029  0.343688
135  0.365405  0.333855  0.335382
150  0.360922  0.327532  0.328219
165  0.353928  0.321444  0.322906
180  0.349107  0.316756  0.318998
195  0.343517  0.312857  0.314917

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         32        32   32
90         32        32   32
105        32        32   32
120        32        32   32
135        32        32   32
150        32        32   32
165        32        32   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.140107  0.127693  0.119828
f15  0.122935  0.116502  0.106557
f26  0.090022  0.078011  0.083026
f27  0.086085  0.098919  0.072917
f12  0.068082  0.082346  0.058643


{'n_train': 1000000, 'num_leaves': 256, 'max_depth': 15}
training XGBoost
EQBIN_dw: 28.253324270248413 seconds
EQBIN_lg: 27.09995150566101 seconds
training LightGBM
LightGBM: 23.666390419006348 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.658978  0.653759  0.653882
15   0.436275  0.397689  0.397984
30   0.364068  0.319342  0.320136
45   0.330131  0.283043  0.284247
60   0.309972  0.265661  0.266184
75   0.294529  0.251078  0.251481
90   0.285495  0.242280  0.241609
105  0.274485  0.234119  0.236488
120  0.267220  0.227768  0.229365
135  0.258687  0.222479  0.225269
150  0.250579  0.218504  0.220956
165  0.245804  0.214100  0.216640
180  0.239562  0.210428  0.211856
195  0.233704  0.206175  0.208866

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.659296  0.654160  0.654291
15   0.439298  0.401931  0.402092
30   0.369188  0.325920  0.326806
45   0.336710  0.291684  0.292814
60   0.317876  0.276088  0.276493
75   0.303677  0.263207  0.263423
90   0.295721  0.256162  0.255162
105  0.285901  0.249801  0.252053
120  0.279843  0.245363  0.246673
135  0.272573  0.242046  0.244567
150  0.265802  0.240092  0.242143
165  0.262552  0.237629  0.239823
180  0.257501  0.235894  0.236836
195  0.253013  0.233382  0.235683

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         256       256  256
15        256       256  256
30        256       256  256
45        256       256  256
60        256       256  256
75        256       256  256
90        256       256  256
105       256       256  256
120       256       256  256
135       256       256  256
150       256       256  256
165       256       256  256
180       256       256  256
195       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f15  0.110624  0.106874  0.096248
f23  0.110058  0.106224  0.106597
f27  0.091593  0.087996  0.059252
f26  0.078340  0.069725  0.071416
f12  0.067197  0.063833  0.049449


{'n_train': 1000000, 'num_leaves': 1024, 'max_depth': 15}
training XGBoost
EQBIN_dw: 56.00496220588684 seconds
EQBIN_lg: 53.50872468948364 seconds
training LightGBM
LightGBM: 40.01816129684448 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.650549  0.643973  0.643954
15   0.379917  0.338724  0.339882
30   0.301987  0.258629  0.259099
45   0.263452  0.222105  0.223123
60   0.243268  0.206296  0.203529
75   0.227790  0.192565  0.192006
90   0.212702  0.181020  0.180344
105  0.202582  0.171697  0.169099
120  0.194347  0.164250  0.160174
135  0.185765  0.156942  0.153828
150  0.176819  0.150650  0.147376
165  0.170091  0.144355  0.140949
180  0.164323  0.138118  0.134671
195  0.157533  0.131431  0.129470

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.651348  0.644941  0.644960
15   0.388126  0.349592  0.350832
30   0.315416  0.276035  0.276559
45   0.281390  0.245225  0.246499
60   0.264951  0.234128  0.232075
75   0.253319  0.226688  0.226859
90   0.242329  0.221518  0.221547
105  0.237006  0.218643  0.216613
120  0.233330  0.217450  0.213708
135  0.229333  0.216016  0.213341
150  0.224727  0.214817  0.212311
165  0.222586  0.213605  0.211028
180  0.221037  0.212242  0.209915
195  0.218470  0.210152  0.209038

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        1024      1024  1024
15       1024      1024  1024
30       1024      1024  1024
45       1024      1024  1024
60       1024       988  1024
75       1024      1024  1024
90       1024      1024  1024
105      1024      1024  1024
120      1024      1024  1024
135      1024      1024  1024
150      1024      1024  1024
165      1024      1024  1024
180      1024      1024  1024
195      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.099805  0.104039  0.097005
f15  0.097814  0.097895  0.089892
f27  0.080228  0.076397  0.054279
f26  0.071959  0.066414  0.063344
f12  0.059452  0.058369  0.051050


{'n_train': 1000000, 'num_leaves': 4096, 'max_depth': 15}
training XGBoost
EQBIN_dw: 114.25734877586365 seconds
EQBIN_lg: 111.2274820804596 seconds
training LightGBM
LightGBM: 62.78660821914673 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.640997  0.636590  0.636563
15   0.317563  0.291948  0.291189
30   0.229187  0.202548  0.203408
45   0.185212  0.164273  0.164536
60   0.167143  0.148091  0.145519
75   0.152685  0.131793  0.131822
90   0.137441  0.122523  0.123754
105  0.127334  0.112892  0.112748
120  0.114207  0.104450  0.103970
135  0.107067  0.096897  0.095949
150  0.099984  0.090378  0.089228
165  0.094205  0.083945  0.082127
180  0.086768  0.078155  0.077026
195  0.078581  0.071887  0.071735

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.643100  0.639137  0.639139
15   0.340330  0.319891  0.319376
30   0.266388  0.248607  0.249423
45   0.235921  0.225026  0.225487
60   0.225254  0.216929  0.215806
75   0.219214  0.211246  0.211246
90   0.214868  0.209586  0.210187
105  0.212827  0.207766  0.207631
120  0.209304  0.206494  0.206145
135  0.208288  0.205194  0.204957
150  0.207166  0.203853  0.203689
165  0.206298  0.202952  0.202253
180  0.204836  0.202244  0.201676
195  0.203333  0.201211  0.201273

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        4096      4096  4096
15       4096      4096  4096
30       4096      4096  4096
45       4096      4096  3854
60       1158      1079  2323
75       4096      2594  1363
90       4096      4096  2272
105      1882      3973  2193
120       943      1552  3998
135      4096      1647  2893
150      2023       788  2159
165      1275      2553  3647
180      3169      3298  3627
195      3639      1608  3051

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.092808  0.096417  0.092821
f15  0.090187  0.090334  0.084031
f27  0.071361  0.070850  0.052220
f26  0.063860  0.062841  0.059345
f31  0.058116  0.056992  0.074797


{'n_train': 1000000, 'num_leaves': 16384, 'max_depth': 15}
training XGBoost
EQBIN_dw: 134.14274978637695 seconds
EQBIN_lg: 132.4269118309021 seconds
training LightGBM
LightGBM: 72.87219023704529 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.634418  0.634418  0.634733
15   0.277973  0.277973  0.276974
30   0.187036  0.187036  0.186760
45   0.151796  0.151796  0.151243
60   0.135743  0.135743  0.136065
75   0.126092  0.126092  0.124645
90   0.114368  0.114368  0.114308
105  0.103327  0.103327  0.103977
120  0.096412  0.096412  0.095224
135  0.087375  0.087375  0.088733
150  0.080501  0.080501  0.082735
165  0.073825  0.073825  0.076002
180  0.069124  0.069124  0.068732
195  0.064252  0.064252  0.065216

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.638379  0.638379  0.638408
15   0.317402  0.317402  0.316311
30   0.247397  0.247397  0.247135
45   0.225187  0.225187  0.224675
60   0.216443  0.216443  0.216716
75   0.213225  0.213225  0.212489
90   0.209711  0.209711  0.210393
105  0.206936  0.206936  0.208238
120  0.205851  0.205851  0.206446
135  0.204210  0.204210  0.205107
150  0.203327  0.203327  0.204240
165  0.202215  0.202215  0.203292
180  0.201898  0.201898  0.201695
195  0.201364  0.201364  0.201450

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        9049      9049  8587
15       7866      7866  8525
30       6273      6273  6161
45       3104      3104  3051
60       1795      1795   885
75       3364      3364  1151
90       2919      2919   962
105      4011      4011  3819
120      3976      3976  3121
135      1924      1924  2887
150      2826      2826  4699
165      2431      2431  3319
180       873       873  3761
195      3867      3867   816

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.095375  0.095375  0.093106
f15  0.089815  0.089815  0.084108
f27  0.070618  0.070618  0.052179
f26  0.062409  0.062409  0.059264
f31  0.056224  0.056224  0.073261


{'n_train': 1000000, 'num_leaves': 32, 'max_depth': 20}
training XGBoost
EQBIN_dw: 16.30952525138855 seconds
EQBIN_lg: 15.548656702041626 seconds
training LightGBM
LightGBM: 13.500432014465332 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.670712  0.668968  0.668840
15   0.519764  0.497579  0.498695
30   0.465616  0.433017  0.436759
45   0.437321  0.400551  0.405310
60   0.416064  0.379374  0.384198
75   0.401602  0.366446  0.368489
90   0.387134  0.356462  0.357139
105  0.376121  0.347640  0.347525
120  0.368930  0.339436  0.340291
135  0.361933  0.331315  0.333814
150  0.357223  0.324844  0.326401
165  0.349977  0.318642  0.319250
180  0.344920  0.313780  0.314687
195  0.339195  0.308742  0.309682

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.670815  0.669090  0.668960
15   0.520467  0.498277  0.499392
30   0.466785  0.434389  0.438104
45   0.439002  0.402278  0.406981
60   0.418094  0.381363  0.386366
75   0.403939  0.368830  0.371027
90   0.389707  0.359072  0.359909
105  0.379062  0.350503  0.350525
120  0.372137  0.342583  0.343529
135  0.365405  0.334745  0.337299
150  0.360922  0.328601  0.330175
165  0.353928  0.322562  0.323401
180  0.349107  0.317854  0.319035
195  0.343517  0.313083  0.314244

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         32        32   32
90         32        32   32
105        32        32   32
120        32        32   32
135        32        32   32
150        32        32   32
165        32        32   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.140107  0.125048  0.119417
f15  0.122935  0.111027  0.105055
f26  0.090022  0.079951  0.084381
f27  0.086085  0.099465  0.072204
f12  0.068082  0.084838  0.059524


{'n_train': 1000000, 'num_leaves': 256, 'max_depth': 20}
training XGBoost
EQBIN_dw: 28.90804433822632 seconds
EQBIN_lg: 26.74857449531555 seconds
training LightGBM
LightGBM: 24.281644344329834 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.658978  0.653759  0.653882
15   0.436275  0.397949  0.398429
30   0.364068  0.319321  0.319392
45   0.330131  0.280924  0.282460
60   0.309972  0.260308  0.260581
75   0.294529  0.246195  0.246320
90   0.285495  0.238251  0.237399
105  0.274485  0.231892  0.231755
120  0.267220  0.225970  0.226808
135  0.258687  0.221754  0.221183
150  0.250579  0.216947  0.217474
165  0.245804  0.212677  0.213359
180  0.239562  0.207560  0.208842
195  0.233704  0.204132  0.205788

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.659296  0.654160  0.654291
15   0.439298  0.402164  0.402700
30   0.369188  0.326011  0.326226
45   0.336710  0.289500  0.291343
60   0.317876  0.270695  0.271149
75   0.303677  0.258203  0.258529
90   0.295721  0.252180  0.251366
105  0.285901  0.247836  0.247761
120  0.279843  0.243872  0.244766
135  0.272573  0.241625  0.240910
150  0.265802  0.238649  0.239234
165  0.262552  0.236353  0.237080
180  0.257501  0.232988  0.234332
195  0.253013  0.231363  0.233253

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         256       256  256
15        256       256  256
30        256       256  256
45        256       256  256
60        256       256  256
75        256       256  256
90        256       256  256
105       256       256  256
120       256       256  256
135       256       256  256
150       256       256  256
165       256       256  256
180       256       256  256
195       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f15  0.110624  0.106380  0.097308
f23  0.110058  0.110898  0.106229
f27  0.091593  0.084986  0.057826
f26  0.078340  0.070899  0.072413
f12  0.067197  0.061171  0.049530


{'n_train': 1000000, 'num_leaves': 1024, 'max_depth': 20}
training XGBoost
EQBIN_dw: 57.04955077171326 seconds
EQBIN_lg: 52.92953848838806 seconds
training LightGBM
LightGBM: 39.530184268951416 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.650549  0.643458  0.643552
15   0.379917  0.337132  0.336633
30   0.301987  0.254547  0.254079
45   0.263452  0.216717  0.217000
60   0.243268  0.196326  0.196308
75   0.227790  0.183981  0.183190
90   0.212702  0.173867  0.173372
105  0.202536  0.164521  0.165280
120  0.194374  0.156230  0.157161
135  0.185182  0.149129  0.149883
150  0.176199  0.142146  0.142416
165  0.168494  0.135063  0.135690
180  0.161549  0.128477  0.129528
195  0.155590  0.123057  0.123928

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.651348  0.644455  0.644528
15   0.388126  0.348313  0.347765
30   0.315416  0.272545  0.272157
45   0.281390  0.240839  0.241094
60   0.264951  0.226687  0.226532
75   0.253319  0.221760  0.220861
90   0.242329  0.218880  0.218523
105  0.236956  0.216367  0.216658
120  0.233646  0.214229  0.214525
135  0.228747  0.212977  0.213259
150  0.224112  0.211425  0.211311
165  0.221090  0.209543  0.210436
180  0.218330  0.208676  0.209740
195  0.216394  0.208059  0.208930

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        1024      1024  1024
15       1024      1024  1024
30       1024      1024  1024
45       1024      1024  1024
60       1024      1024  1024
75       1024      1024  1024
90       1024      1024  1024
105      1024      1024  1024
120      1024      1024  1024
135      1024      1024  1024
150      1024      1024  1024
165      1024      1024  1024
180      1024      1024  1024
195      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.098998  0.103551  0.095200
f15  0.098559  0.098764  0.090630
f27  0.079820  0.074899  0.053327
f26  0.071173  0.063356  0.063990
f12  0.059594  0.056441  0.051201


{'n_train': 1000000, 'num_leaves': 4096, 'max_depth': 20}
training XGBoost
EQBIN_dw: 141.9930648803711 seconds
EQBIN_lg: 148.96680283546448 seconds
training LightGBM
LightGBM: 89.96422004699707 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.640997  0.633776  0.633688
15   0.317563  0.277907  0.277983
30   0.229187  0.185827  0.185798
45   0.185212  0.143685  0.144559
60   0.157450  0.119812  0.120887
75   0.139900  0.103525  0.105074
90   0.124637  0.089445  0.090954
105  0.109539  0.077772  0.079095
120  0.097830  0.067224  0.068103
135  0.087289  0.059082  0.059748
150  0.078557  0.050006  0.050455
165  0.070313  0.042528  0.042128
180  0.062024  0.035838  0.035647
195  0.056294  0.030475  0.030184

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.643100  0.636476  0.636473
15   0.340330  0.308290  0.308334
30   0.266388  0.237060  0.236972
45   0.235921  0.214238  0.214913
60   0.220722  0.206273  0.206334
75   0.215195  0.203469  0.204114
90   0.211442  0.201209  0.201781
105  0.207278  0.199788  0.200034
120  0.204773  0.198383  0.198672
135  0.203375  0.197834  0.198061
150  0.202547  0.197355  0.197463
165  0.201730  0.197240  0.197259
180  0.201061  0.197419  0.197432
195  0.200473  0.198022  0.197898

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        4096      4096  4096
15       4096      4096  4096
30       4096      4096  4096
45       4096      4096  4096
60       4096      3702  4096
75       4096      3173  4096
90       4096      2853  4096
105      4096      3091  4096
120      4096      4096  4096
135      2935      4096  2528
150      2664      4096  4096
165      3486      4096  4096
180      4096      4096  4096
195      4096      4096  2998

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.089533  0.093178  0.089273
f15  0.088024  0.089299  0.082002
f27  0.069919  0.066631  0.051101
f26  0.062530  0.061258  0.058229
f31  0.058298  0.057023  0.073007


{'n_train': 1000000, 'num_leaves': 16384, 'max_depth': 20}
training XGBoost
EQBIN_dw: 260.0459563732147 seconds
EQBIN_lg: 253.1720588207245 seconds
training LightGBM
LightGBM: 133.42227339744568 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.628454  0.624718  0.624990
15   0.234763  0.215275  0.217303
30   0.129398  0.113289  0.114691
45   0.087908  0.078545  0.078694
60   0.068822  0.063355  0.061245
75   0.060333  0.055343  0.052799
90   0.051742  0.046615  0.044694
105  0.042253  0.038134  0.037841
120  0.034349  0.032529  0.029889
135  0.027040  0.026457  0.024112
150  0.022265  0.021426  0.019996
165  0.018087  0.017612  0.017176
180  0.015168  0.014861  0.014759
195  0.013296  0.012840  0.012792

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.635051  0.632634  0.632506
15   0.300698  0.293388  0.293272
30   0.230998  0.227207  0.227142
45   0.211471  0.210498  0.210149
60   0.204740  0.204962  0.203544
75   0.202836  0.202900  0.201779
90   0.201568  0.201753  0.200527
105  0.200776  0.200924  0.200189
120  0.200840  0.200766  0.200090
135  0.201148  0.201254  0.200909
150  0.202082  0.202106  0.201933
165  0.203223  0.203340  0.203082
180  0.204514  0.204519  0.204326
195  0.205864  0.205975  0.205793

Leaf counts
     EQBIN_dw  EQBIN_lg    LGB
0       16384     16384  16384
15      16384     16384  16384
30      16384     15421  15318
45       7877      8527   7813
60       6504      3679   5129
75       6294      3126   6239
90       4979      7633   6139
105      7148      3918   6356
120      8113      7504   8271
135      5417      7686   4620
150      5395      8448   7187
165      7648      5256   3320
180      4666      6114   5136
195      2240      4386   4447

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.090659  0.092100  0.090572
f15  0.087683  0.086713  0.082027
f27  0.066662  0.065176  0.050066
f26  0.059875  0.059718  0.056790
f31  0.056090  0.055479  0.070682


{'n_train': 2000000, 'num_leaves': 32, 'max_depth': 5}
training XGBoost
EQBIN_dw: 30.23603582382202 seconds
EQBIN_lg: 30.54032039642334 seconds
training LightGBM
LightGBM: 23.12584352493286 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.668197  0.668197  0.668792
15   0.502499  0.502499  0.504351
30   0.458417  0.458417  0.460120
45   0.428591  0.428591  0.431825
60   0.412420  0.412420  0.413065
75   0.397671  0.397671  0.395972
90   0.387534  0.387534  0.383286
105  0.378149  0.378149  0.374027
120  0.372052  0.372052  0.365721
135  0.363631  0.363631  0.359936
150  0.356775  0.356775  0.351934
165  0.351817  0.351817  0.347529
180  0.342262  0.342262  0.339184
195  0.337286  0.337286  0.332808

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.668257  0.668257  0.668860
15   0.503355  0.503355  0.505316
30   0.459662  0.459662  0.461592
45   0.430273  0.430273  0.433632
60   0.414352  0.414352  0.415190
75   0.399917  0.399917  0.398316
90   0.389928  0.389928  0.385778
105  0.380722  0.380722  0.376646
120  0.374797  0.374797  0.368496
135  0.366580  0.366580  0.362844
150  0.359827  0.359827  0.354928
165  0.354992  0.354992  0.350621
180  0.345515  0.345515  0.342442
195  0.340667  0.340667  0.336205

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         31        31   32
90         27        27   32
105        32        32   32
120        32        32   32
135        32        32   32
150        32        32   32
165        32        32   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.130517  0.130517  0.116585
f29  0.094960  0.094960  0.097067
f16  0.086271  0.086271  0.081918
f31  0.080593  0.080593  0.036486
f12  0.069270  0.069270  0.059917


{'n_train': 2000000, 'num_leaves': 32, 'max_depth': 10}
training XGBoost
EQBIN_dw: 30.075490951538086 seconds
EQBIN_lg: 28.66444969177246 seconds
training LightGBM
LightGBM: 26.409107208251953 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.668197  0.667584  0.667479
15   0.502499  0.492801  0.492586
30   0.458417  0.435045  0.433973
45   0.428591  0.402957  0.403211
60   0.412332  0.382094  0.382484
75   0.397652  0.370901  0.368490
90   0.387497  0.361233  0.356052
105  0.378310  0.350306  0.347458
120  0.368572  0.339882  0.338753
135  0.360008  0.331246  0.331536
150  0.353769  0.324822  0.325243
165  0.349606  0.319309  0.317694
180  0.344543  0.314639  0.312886
195  0.337779  0.311072  0.307084

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.668257  0.667658  0.667597
15   0.503355  0.493941  0.493720
30   0.459662  0.436595  0.435456
45   0.430273  0.404880  0.405039
60   0.414270  0.384326  0.384516
75   0.399854  0.373299  0.370750
90   0.389975  0.363858  0.358529
105  0.381044  0.353100  0.350085
120  0.371437  0.342698  0.341536
135  0.362940  0.334214  0.334439
150  0.356809  0.327931  0.328246
165  0.352735  0.322511  0.320871
180  0.347761  0.318004  0.316148
195  0.341143  0.314555  0.310520

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         32        32   32
90         32        32   32
105        32        32   32
120        32        32   32
135        32        32   32
150        32        32   32
165        32        32   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.127185  0.138336  0.111883
f29  0.096980  0.096677  0.096881
f16  0.087717  0.102263  0.086651
f31  0.074015  0.075692  0.037116
f12  0.066929  0.065681  0.061936


{'n_train': 2000000, 'num_leaves': 256, 'max_depth': 10}
training XGBoost
EQBIN_dw: 48.634493589401245 seconds
EQBIN_lg: 51.67733120918274 seconds
training LightGBM
LightGBM: 47.817654609680176 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.656158  0.652710  0.651779
15   0.416301  0.395077  0.394060
30   0.351350  0.320231  0.322351
45   0.323185  0.296724  0.296705
60   0.307805  0.279908  0.277630
75   0.297070  0.263889  0.262463
90   0.282135  0.251113  0.252029
105  0.271487  0.238350  0.239573
120  0.262332  0.229551  0.230785
135  0.253706  0.221471  0.222859
150  0.243264  0.215901  0.217705
165  0.236092  0.211969  0.214789
180  0.230552  0.206191  0.209184
195  0.227198  0.201775  0.204043

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.656404  0.653028  0.652045
15   0.419014  0.398354  0.397242
30   0.355150  0.324895  0.327059
45   0.327888  0.302424  0.302278
60   0.313217  0.286330  0.284091
75   0.303062  0.270972  0.269662
90   0.288764  0.258979  0.259941
105  0.278745  0.247014  0.248217
120  0.270235  0.239049  0.240260
135  0.262297  0.231796  0.233192
150  0.252516  0.227110  0.228815
165  0.246085  0.224040  0.226841
180  0.241248  0.219069  0.222077
195  0.238583  0.215493  0.217705

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         256       256  256
15        256       256  256
30        256       256  256
45        256       256  256
60        256       256  256
75        256       256  256
90        256       256  256
105       256       256  256
120       256       256  256
135       256       256  256
150       256       256  256
165       256       256  256
180       256       256  256
195       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.133665  0.136001  0.099881
f29  0.096434  0.097866  0.087777
f16  0.095221  0.092049  0.078846
f24  0.070382  0.067726  0.128869
f9   0.066337  0.063649  0.074207


{'n_train': 2000000, 'num_leaves': 1024, 'max_depth': 10}
training XGBoost
EQBIN_dw: 70.02881264686584 seconds
EQBIN_lg: 68.45753717422485 seconds
training LightGBM
LightGBM: 59.28399872779846 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.648192  0.648192  0.647304
15   0.364879  0.364879  0.365585
30   0.291926  0.291926  0.292255
45   0.270327  0.270327  0.267045
60   0.251356  0.251356  0.253353
75   0.239048  0.239048  0.233003
90   0.223576  0.223576  0.220634
105  0.214349  0.214349  0.211728
120  0.205396  0.205396  0.202730
135  0.200692  0.200692  0.195827
150  0.191552  0.191552  0.192544
165  0.183042  0.183042  0.184647
180  0.180105  0.180105  0.178429
195  0.177657  0.177657  0.173795

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.648686  0.648686  0.647815
15   0.370408  0.370408  0.370935
30   0.300284  0.300284  0.300326
45   0.280182  0.280182  0.276627
60   0.262590  0.262590  0.264261
75   0.251719  0.251719  0.245617
90   0.237951  0.237951  0.234962
105  0.230147  0.230147  0.227687
120  0.222882  0.222882  0.220441
135  0.219705  0.219705  0.215239
150  0.212423  0.212423  0.213542
165  0.205749  0.205749  0.207464
180  0.204338  0.204338  0.203141
195  0.203393  0.203393  0.200051

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         985       985  982
15        985       985  960
30        858       858  919
45        545       545  619
60        428       428  379
75        841       841  797
90        819       819  880
105       591       591  707
120       486       486  616
135       651       651  416
150       873       873  737
165       817       817  879
180       424       424  742
195       548       548  570

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.131471  0.131471  0.097288
f29  0.097900  0.097900  0.089153
f16  0.088010  0.088010  0.076665
f24  0.072497  0.072497  0.123736
f9   0.062764  0.062764  0.071013


{'n_train': 2000000, 'num_leaves': 32, 'max_depth': 15}
training XGBoost
EQBIN_dw: 30.44684672355652 seconds
EQBIN_lg: 29.161646604537964 seconds
training LightGBM
LightGBM: 26.373711347579956 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.668197  0.667584  0.667479
15   0.502499  0.492764  0.491935
30   0.458417  0.432709  0.432188
45   0.428591  0.401036  0.402481
60   0.412332  0.381574  0.381095
75   0.397652  0.368847  0.369144
90   0.387497  0.355976  0.356675
105  0.378310  0.345112  0.347515
120  0.368572  0.336908  0.338233
135  0.360008  0.331494  0.331846
150  0.353769  0.325300  0.323982
165  0.349606  0.317515  0.318516
180  0.344543  0.312189  0.311204
195  0.337779  0.306237  0.307261

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.668257  0.667658  0.667597
15   0.503355  0.493906  0.493109
30   0.459662  0.434252  0.433647
45   0.430273  0.402868  0.404301
60   0.414270  0.383712  0.383136
75   0.399854  0.371223  0.371385
90   0.389975  0.358468  0.359087
105  0.381044  0.347838  0.350122
120  0.371437  0.339798  0.341065
135  0.362940  0.334504  0.334815
150  0.356809  0.328406  0.327114
165  0.352735  0.320802  0.321812
180  0.347761  0.315648  0.314658
195  0.341143  0.309779  0.310849

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         32        32   32
90         32        32   32
105        32        32   32
120        32        32   32
135        32        32   32
150        32        32   32
165        32        32   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.127185  0.137166  0.116649
f29  0.096980  0.100548  0.097949
f16  0.087717  0.093916  0.085372
f31  0.074015  0.085031  0.035618
f12  0.066929  0.057137  0.058591


{'n_train': 2000000, 'num_leaves': 256, 'max_depth': 15}
training XGBoost
EQBIN_dw: 48.946234941482544 seconds
EQBIN_lg: 45.651583433151245 seconds
training LightGBM
LightGBM: 48.43707823753357 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.656158  0.652332  0.651208
15   0.416301  0.389403  0.389860
30   0.351350  0.310204  0.309492
45   0.323185  0.279427  0.281829
60   0.307805  0.257062  0.260335
75   0.297070  0.242686  0.243533
90   0.282135  0.233639  0.232481
105  0.271487  0.226250  0.224319
120  0.262332  0.219441  0.217465
135  0.253706  0.215145  0.212750
150  0.243264  0.210053  0.209232
165  0.236092  0.204692  0.205744
180  0.230552  0.201650  0.201731
195  0.227198  0.198414  0.199107

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.656404  0.652657  0.651481
15   0.419014  0.392599  0.393096
30   0.355150  0.314945  0.314360
45   0.327888  0.285217  0.287844
60   0.313217  0.263842  0.267147
75   0.303062  0.250386  0.251208
90   0.288764  0.242166  0.241061
105  0.278745  0.235564  0.233799
120  0.270235  0.229626  0.227877
135  0.262297  0.226241  0.224076
150  0.252516  0.222083  0.221437
165  0.246085  0.217644  0.218918
180  0.241248  0.215639  0.215667
195  0.238583  0.213325  0.213959

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         256       256  256
15        256       256  256
30        256       256  256
45        256       256  256
60        256       256  256
75        256       256  256
90        256       256  256
105       256       256  256
120       256       256  256
135       256       256  256
150       256       256  256
165       256       256  256
180       256       256  256
195       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.133665  0.142190  0.099271
f29  0.096434  0.095742  0.087326
f16  0.095221  0.090654  0.078728
f24  0.070382  0.068764  0.128522
f9   0.066337  0.062362  0.072408


{'n_train': 2000000, 'num_leaves': 1024, 'max_depth': 15}
training XGBoost
EQBIN_dw: 82.91311120986938 seconds
EQBIN_lg: 80.71360993385315 seconds
training LightGBM
LightGBM: 69.37216234207153 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.647957  0.642051  0.641569
15   0.364843  0.329033  0.328001
30   0.291531  0.247037  0.246351
45   0.262601  0.218827  0.219192
60   0.241492  0.201040  0.201291
75   0.223358  0.188844  0.187092
90   0.208667  0.178791  0.177429
105  0.198864  0.172384  0.171602
120  0.191361  0.166199  0.166671
135  0.183881  0.162036  0.160279
150  0.175894  0.156325  0.155653
165  0.170904  0.151755  0.152148
180  0.166972  0.146606  0.148373
195  0.161532  0.142182  0.144454

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.648459  0.642706  0.642215
15   0.370401  0.335989  0.335014
30   0.300153  0.257969  0.257184
45   0.273498  0.232734  0.233035
60   0.254403  0.217737  0.217809
75   0.238466  0.208457  0.206466
90   0.225990  0.201775  0.200108
105  0.218446  0.198791  0.197796
120  0.213346  0.195900  0.196189
135  0.208342  0.194933  0.192994
150  0.202814  0.192380  0.191435
165  0.200317  0.190673  0.191106
180  0.198599  0.188419  0.190142
195  0.195623  0.186741  0.189252

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        1024      1024  1024
15       1024      1024  1024
30       1024      1024  1024
45       1024      1024  1024
60       1024      1024  1024
75       1024      1024  1024
90       1024      1024  1024
105      1024      1024  1024
120      1024      1024  1024
135      1024      1024  1024
150      1024      1024  1024
165      1024      1024  1024
180      1024      1024  1024
195      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.120410  0.128027  0.093003
f29  0.093475  0.088748  0.085148
f16  0.084261  0.084880  0.075484
f24  0.074033  0.073298  0.122135
f8   0.062958  0.058484  0.089409


{'n_train': 2000000, 'num_leaves': 4096, 'max_depth': 15}
training XGBoost
EQBIN_dw: 159.02288150787354 seconds
EQBIN_lg: 157.13384079933167 seconds
training LightGBM
LightGBM: 116.418377161026 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.639568  0.633948  0.633987
15   0.313271  0.282930  0.282694
30   0.233586  0.199557  0.199075
45   0.198225  0.166658  0.167920
60   0.177924  0.150070  0.150897
75   0.156173  0.138522  0.137273
90   0.143805  0.130463  0.129585
105  0.135069  0.122695  0.122993
120  0.126857  0.115850  0.117070
135  0.119711  0.109853  0.107921
150  0.114728  0.103052  0.100369
165  0.109073  0.096128  0.095702
180  0.101888  0.090397  0.090831
195  0.095594  0.084521  0.085996

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.640793  0.635462  0.635566
15   0.326588  0.299185  0.299130
30   0.254553  0.225802  0.225571
45   0.226202  0.201993  0.203312
60   0.211672  0.191832  0.192959
75   0.196742  0.186291  0.185738
90   0.190958  0.184805  0.183662
105  0.188376  0.182554  0.182493
120  0.185476  0.181184  0.181143
135  0.183229  0.179229  0.178189
150  0.182545  0.177351  0.175841
165  0.181263  0.175964  0.175878
180  0.178798  0.174955  0.175377
195  0.177058  0.173734  0.174198

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        4096      4096  4096
15       4096      4096  4096
30       4096      4096  4096
45       4096      4096  4096
60       4096      4096  1672
75       4096      1622  2924
90       4096      4096   943
105      4096      3796  1914
120      4023      1021  2330
135      4096      2116  4096
150      1010      3906  1138
165       579      2417  1620
180      4096      4096  4096
195      3970      3196  2521

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.109821  0.112321  0.089545
f29  0.086451  0.084541  0.080371
f16  0.080207  0.078688  0.071986
f24  0.075662  0.074335  0.116029
f8   0.062408  0.061281  0.086431


{'n_train': 2000000, 'num_leaves': 16384, 'max_depth': 15}
training XGBoost
EQBIN_dw: 207.14127206802368 seconds
EQBIN_lg: 203.8506691455841 seconds
training LightGBM
LightGBM: 132.05842471122742 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.630933  0.630933  0.631177
15   0.262745  0.262745  0.263911
30   0.178629  0.178629  0.176878
45   0.149653  0.149653  0.149933
60   0.137552  0.137552  0.135538
75   0.123648  0.123648  0.120396
90   0.116654  0.116654  0.112703
105  0.105837  0.105837  0.105360
120  0.098793  0.098793  0.099829
135  0.093018  0.093018  0.092721
150  0.086149  0.086149  0.088181
165  0.080972  0.080972  0.082439
180  0.077084  0.077084  0.074702
195  0.070767  0.070767  0.070527

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.633762  0.633762  0.633909
15   0.291556  0.291556  0.291711
30   0.220845  0.220845  0.219088
45   0.200232  0.200232  0.200367
60   0.192587  0.192587  0.191439
75   0.185850  0.185850  0.183987
90   0.183244  0.183244  0.181651
105  0.179003  0.179003  0.179828
120  0.177503  0.177503  0.179320
135  0.176184  0.176184  0.177326
150  0.174451  0.174451  0.176593
165  0.174149  0.174149  0.175314
180  0.173556  0.173556  0.173132
195  0.172083  0.172083  0.172386

Leaf counts
     EQBIN_dw  EQBIN_lg    LGB
0       12370     12370  11982
15      11097     11097  12292
30       6847      6847   8094
45       5307      5307   3682
60       2695      2695   9423
75       3224      3224   8289
90        981       981   5384
105      4507      4507   2318
120      1272      1272    676
135      4200      4200    773
150      2617      2617   2430
165      3059      3059   3640
180      2061      2061   4593
195      7533      7533   2423

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.108302  0.108302  0.089999
f29  0.083409  0.083409  0.079802
f16  0.078193  0.078193  0.071341
f24  0.073979  0.073979  0.114234
f8   0.061198  0.061198  0.085216


{'n_train': 2000000, 'num_leaves': 32, 'max_depth': 20}
training XGBoost
EQBIN_dw: 30.441197156906128 seconds
EQBIN_lg: 28.65040922164917 seconds
training LightGBM
LightGBM: 26.168627977371216 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.668197  0.667584  0.667479
15   0.502499  0.492764  0.491935
30   0.458417  0.433222  0.432188
45   0.428591  0.401738  0.402477
60   0.412332  0.381896  0.379921
75   0.397652  0.367902  0.366945
90   0.387497  0.356728  0.353551
105  0.378310  0.348882  0.344919
120  0.368572  0.340144  0.338169
135  0.360008  0.332757  0.330754
150  0.353769  0.325746  0.325658
165  0.349606  0.319835  0.320465
180  0.344543  0.315430  0.314185
195  0.337779  0.309952  0.307976

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.668257  0.667658  0.667597
15   0.503355  0.493906  0.493109
30   0.459662  0.434793  0.433647
45   0.430273  0.403679  0.404300
60   0.414270  0.384145  0.382006
75   0.399854  0.370442  0.369270
90   0.389975  0.359480  0.356086
105  0.381044  0.351759  0.347650
120  0.371437  0.343178  0.341063
135  0.362940  0.335944  0.333800
150  0.356809  0.329055  0.328793
165  0.352735  0.323326  0.323691
180  0.347761  0.319031  0.317519
195  0.341143  0.313676  0.311469

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0          32        32   32
15         32        32   32
30         32        32   32
45         32        32   32
60         32        32   32
75         32        32   32
90         32        32   32
105        32        32   32
120        32        32   32
135        32        32   32
150        32        32   32
165        32        32   32
180        32        32   32
195        32        32   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.127185  0.139808  0.115819
f29  0.096980  0.094510  0.097201
f16  0.087717  0.088024  0.084202
f31  0.074015  0.082002  0.033879
f12  0.066929  0.066952  0.059828


{'n_train': 2000000, 'num_leaves': 256, 'max_depth': 20}
training XGBoost
EQBIN_dw: 48.34335541725159 seconds
EQBIN_lg: 44.637067556381226 seconds
training LightGBM
LightGBM: 49.03743004798889 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.656158  0.652332  0.651194
15   0.416301  0.390072  0.389215
30   0.351350  0.307483  0.307969
45   0.323185  0.271729  0.273141
60   0.307805  0.250788  0.253333
75   0.297070  0.239523  0.241645
90   0.282135  0.230279  0.231909
105  0.271487  0.221768  0.222220
120  0.262332  0.216983  0.215063
135  0.253706  0.210671  0.210620
150  0.243264  0.206110  0.206169
165  0.236092  0.201668  0.202626
180  0.230552  0.199470  0.199018
195  0.227198  0.197128  0.194679

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.656404  0.652657  0.651466
15   0.419014  0.393276  0.392438
30   0.355150  0.312305  0.312857
45   0.327888  0.277716  0.279107
60   0.313217  0.257691  0.260192
75   0.303062  0.247327  0.249381
90   0.288764  0.238973  0.240475
105  0.278745  0.231277  0.231628
120  0.270235  0.227369  0.225336
135  0.262297  0.221940  0.221863
150  0.252516  0.218310  0.218331
165  0.246085  0.214776  0.215757
180  0.241248  0.213599  0.213127
195  0.238583  0.212206  0.209731

Leaf counts
     EQBIN_dw  EQBIN_lg  LGB
0         256       256  256
15        256       256  256
30        256       256  256
45        256       256  256
60        256       256  256
75        256       256  256
90        256       256  256
105       256       256  256
120       256       256  256
135       256       256  256
150       256       256  256
165       256       256  256
180       256       256  256
195       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.133665  0.139886  0.097955
f29  0.096434  0.092662  0.085761
f16  0.095221  0.088697  0.081260
f24  0.070382  0.070674  0.126405
f9   0.066337  0.062238  0.074380


{'n_train': 2000000, 'num_leaves': 1024, 'max_depth': 20}
training XGBoost
EQBIN_dw: 83.17439389228821 seconds
EQBIN_lg: 71.8512315750122 seconds
training LightGBM
LightGBM: 67.98218560218811 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.647957  0.641910  0.641413
15   0.364843  0.327983  0.328675
30   0.291531  0.242923  0.244298
45   0.262601  0.209299  0.209565
60   0.241492  0.189310  0.190855
75   0.223358  0.178215  0.177900
90   0.208667  0.172201  0.172284
105  0.198864  0.166643  0.167586
120  0.191361  0.160857  0.161393
135  0.183881  0.155487  0.156361
150  0.175894  0.151459  0.150934
165  0.170904  0.147283  0.147022
180  0.166730  0.143837  0.142238
195  0.159862  0.140429  0.138765

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.648459  0.642562  0.642060
15   0.370401  0.335102  0.335665
30   0.300153  0.253984  0.255394
45   0.273498  0.223411  0.223878
60   0.254403  0.206357  0.208107
75   0.238466  0.198666  0.198499
90   0.225990  0.196271  0.196741
105  0.218446  0.194467  0.195825
120  0.213346  0.192255  0.193127
135  0.208342  0.190365  0.191197
150  0.202814  0.189819  0.189039
165  0.200317  0.188932  0.188287
180  0.198392  0.188424  0.186693
195  0.193844  0.188016  0.185985

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        1024      1024  1024
15       1024      1024  1024
30       1024      1024  1024
45       1024      1024  1024
60       1024      1024  1024
75       1024      1024  1024
90       1024      1024  1024
105      1024      1024  1024
120      1024      1024  1024
135      1024      1024  1024
150      1024      1024  1024
165      1024      1024  1024
180      1024      1024  1024
195      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.121470  0.126762  0.093182
f29  0.093262  0.091559  0.084571
f16  0.084588  0.084249  0.075069
f24  0.073451  0.072932  0.121235
f8   0.062614  0.060079  0.090615


{'n_train': 2000000, 'num_leaves': 4096, 'max_depth': 20}
training XGBoost
EQBIN_dw: 179.76432466506958 seconds
EQBIN_lg: 180.98852515220642 seconds
training LightGBM
LightGBM: 131.78722190856934 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.639568  0.632341  0.632336
15   0.313271  0.273735  0.273573
30   0.233586  0.186708  0.185923
45   0.198093  0.152459  0.150712
60   0.173090  0.133472  0.132075
75   0.153095  0.118241  0.118056
90   0.139472  0.107197  0.107507
105  0.130807  0.098607  0.098537
120  0.120023  0.091047  0.090732
135  0.109868  0.084440  0.083136
150  0.102103  0.075567  0.075833
165  0.095141  0.068567  0.068982
180  0.088646  0.062483  0.062604
195  0.082366  0.056444  0.057873

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.640793  0.633997  0.633994
15   0.326588  0.291041  0.290932
30   0.254553  0.214966  0.214197
45   0.226137  0.191327  0.189770
60   0.207948  0.182071  0.181137
75   0.195296  0.177586  0.177271
90   0.188616  0.175777  0.176015
105  0.186630  0.174563  0.174590
120  0.182771  0.173474  0.173725
135  0.179429  0.172978  0.173085
150  0.177807  0.171085  0.171799
165  0.176607  0.169982  0.170426
180  0.175692  0.169322  0.169790
195  0.175199  0.168851  0.169530

Leaf counts
     EQBIN_dw  EQBIN_lg   LGB
0        4096      4096  4096
15       4096      4096  4096
30       4096      4096  4096
45       4096      4096  4096
60       4096      4096  4096
75       4096      4096  4096
90       4096      3821  4096
105      4096      1000  4096
120      4096      1824  4096
135      4096      2148  4096
150      4096      4096  4096
165      3490      4096  3084
180      4096      4096  4096
195      4096      4096  4096

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.106217  0.109633  0.087486
f29  0.084707  0.083104  0.078489
f16  0.078629  0.076426  0.071003
f24  0.077138  0.073446  0.112226
f8   0.062991  0.059580  0.086101


{'n_train': 2000000, 'num_leaves': 16384, 'max_depth': 20}
training XGBoost
EQBIN_dw: 397.37513422966003 seconds
EQBIN_lg: 381.5458061695099 seconds
training LightGBM
LightGBM: 235.96768760681152 seconds

binary_logloss train
     EQBIN_dw  EQBIN_lg       LGB
0    0.628583  0.623934  0.624431
15   0.247857  0.220849  0.222046
30   0.152745  0.124646  0.125403
45   0.111355  0.089135  0.089502
60   0.090130  0.076136  0.075420
75   0.076412  0.066157  0.065664
90   0.062584  0.060797  0.058490
105  0.054098  0.050320  0.048288
120  0.047284  0.041752  0.039602
135  0.037107  0.033672  0.031757
150  0.029694  0.027965  0.026409
165  0.024126  0.022936  0.022293
180  0.020502  0.018807  0.018637
195  0.017097  0.015552  0.015665

binary_logloss valid
     EQBIN_dw  EQBIN_lg       LGB
0    0.632183  0.628600  0.628848
15   0.284126  0.267789  0.267843
30   0.209831  0.197196  0.197095
45   0.186327  0.179140  0.179167
60   0.177504  0.173929  0.173740
75   0.174096  0.171244  0.171468
90   0.171125  0.170688  0.170281
105  0.170100  0.168802  0.169097
120  0.169689  0.168383  0.168640
135  0.168941  0.168034  0.168340
150  0.168830  0.168094  0.168516
165  0.169351  0.168509  0.168940
180  0.169909  0.169262  0.169730
195  0.170715  0.170242  0.170779

Leaf counts
     EQBIN_dw  EQBIN_lg    LGB
0       16384     16384  16384
15      16384     16384  16384
30      16384     16384  16384
45       9371     14553  13732
60      16384      4693   6497
75      13524      7358   2616
90      15480     12039   3040
105      7941      6561  12816
120      3015     10691  14335
135     11508      8199  14969
150     10005     12594  11153
165      9296     13774   4916
180      5952      9225   8677
195     11737      8568   9671

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.098749  0.101787  0.085823
f29  0.079558  0.079651  0.076589
f16  0.074222  0.073522  0.069255
f24  0.073957  0.072686  0.107918
f8   0.061899  0.061199  0.083860


                             Time(sec)                        Ratio             
                              EQBIN_dw EQBIN_lg    LGB EQBIN_dw/LGB EQBIN_lg/LGB
n_train num_leaves max_depth                                                    
500000  32         5               8.4      8.0    5.5          1.5          1.5
                   10              8.4      8.0    6.3          1.3          1.3
                   15              8.3      8.0    6.4          1.3          1.3
                   20              8.3      8.1    6.2          1.3          1.3
        256        10             18.3     17.7   11.7          1.6          1.5
                   15             18.2     17.3   12.1          1.5          1.4
                   20             18.0     16.8   12.2          1.5          1.4
        1024       10             26.1     25.7   14.1          1.8          1.8
                   15             42.6     41.4   23.3          1.8          1.8
                   20             43.4     42.2   25.0          1.7          1.7
        4096       15             77.2     76.3   35.5          2.2          2.1
                   20            112.6    113.8   54.8          2.1          2.1
        16384      15             84.3     83.8   40.1          2.1          2.1
                   20            149.4    148.7   67.7          2.2          2.2
1000000 32         5              16.2     15.7   11.3          1.4          1.4
                   10             16.0     15.5   13.3          1.2          1.2
                   15             15.7     15.8   13.5          1.2          1.2
                   20             16.3     15.5   13.5          1.2          1.2
        256        10             29.2     28.3   23.4          1.3          1.2
                   15             28.3     27.1   23.7          1.2          1.1
                   20             28.9     26.7   24.3          1.2          1.1
        1024       10             42.2     40.9   29.0          1.5          1.4
                   15             56.0     53.5   40.0          1.4          1.3
                   20             57.0     52.9   39.5          1.4          1.3
        4096       15            114.3    111.2   62.8          1.8          1.8
                   20            142.0    149.0   90.0          1.6          1.7
        16384      15            134.1    132.4   72.9          1.8          1.8
                   20            260.0    253.2  133.4          1.9          1.9
2000000 32         5              30.2     30.5   23.1          1.3          1.3
                   10             30.1     28.7   26.4          1.1          1.1
                   15             30.4     29.2   26.4          1.2          1.1
                   20             30.4     28.7   26.2          1.2          1.1
        256        10             48.6     51.7   47.8          1.0          1.1
                   15             48.9     45.7   48.4          1.0          0.9
                   20             48.3     44.6   49.0          1.0          0.9
        1024       10             70.0     68.5   59.3          1.2          1.2
                   15             82.9     80.7   69.4          1.2          1.2
                   20             83.2     71.9   68.0          1.2          1.1
        4096       15            159.0    157.1  116.4          1.4          1.3
                   20            179.8    181.0  131.8          1.4          1.4
        16384      15            207.1    203.9  132.1          1.6          1.5
                   20            397.4    381.5  236.0          1.7          1.6

Logloss
                              EQBIN_dw  EQBIN_lg      LGB  EQBIN_lg-LGB
n_train num_leaves max_depth                                           
500000  32         5           0.36296   0.36296  0.36228       0.00067
                   10          0.36084   0.32989  0.33306      -0.00317
                   15          0.36084   0.32803  0.32541       0.00262
                   20          0.36084   0.33081  0.33304      -0.00223
        256        10          0.26726   0.25364  0.25515      -0.00151
                   15          0.26803   0.25459  0.25560      -0.00101
                   20          0.26803   0.25180  0.25049       0.00131
        1024       10          0.24845   0.24845  0.24849      -0.00004
                   15          0.23223   0.22722  0.22681       0.00040
                   20          0.23134   0.22405  0.22303       0.00102
        4096       15          0.22261   0.22023  0.22109      -0.00086
                   20          0.22044   0.21882  0.21887      -0.00005
        16384      15          0.22148   0.22148  0.21995       0.00153
                   20          0.22389   0.22389  0.22266       0.00123
1000000 32         5           0.34270   0.34270  0.34309      -0.00039
                   10          0.34286   0.31639  0.31487       0.00152
                   15          0.34286   0.31226  0.31393      -0.00167
                   20          0.34286   0.31172  0.31282      -0.00110
        256        10          0.24967   0.23941  0.23956      -0.00015
                   15          0.25168   0.23304  0.23539      -0.00235
                   20          0.25168   0.23121  0.23212      -0.00092
        1024       10          0.22757   0.22757  0.22549       0.00208
                   15          0.21827   0.21009  0.20857       0.00153
                   20          0.21609   0.20795  0.20859      -0.00064
        4096       15          0.20322   0.20123  0.20092       0.00031
                   20          0.20022   0.19817  0.19807       0.00010
        16384      15          0.20134   0.20134  0.20146      -0.00012
                   20          0.20619   0.20639  0.20622       0.00016
2000000 32         5           0.33979   0.33979  0.33423       0.00555
                   10          0.33893   0.31382  0.30907       0.00474
                   15          0.33893   0.30837  0.30984      -0.00147
                   20          0.33893   0.31194  0.31050       0.00144
        256        10          0.23744   0.21472  0.21724      -0.00252
                   15          0.23744   0.21310  0.21347      -0.00037
                   20          0.23744   0.21204  0.20879       0.00325
        1024       10          0.20274   0.20274  0.19997       0.00277
                   15          0.19503   0.18647  0.18898      -0.00251
                   20          0.19324   0.18807  0.18549       0.00258
        4096       15          0.17666   0.17378  0.17400      -0.00022
                   20          0.17502   0.16870  0.16935      -0.00064
        16384      15          0.17191   0.17191  0.17236      -0.00045
                   20          0.17095   0.17057  0.17110      -0.00053

Done: 8556.412877321243 seconds
